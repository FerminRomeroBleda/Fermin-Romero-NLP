{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de detección de enlaces spam\n",
    "\n",
    "---\n",
    "\n",
    "Queremos implementar un sistema que sea capaz de detectar automáticamente si una página web contiene spam o no basándonos en su URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 1: Carga del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.hvper.com/</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://briefingday.com/fan</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  is_spam\n",
       "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
       "1                             https://www.hvper.com/     True\n",
       "2                 https://briefingday.com/m/v4n3i4f3     True\n",
       "3   https://briefingday.com/n/20200618/m#commentform    False\n",
       "4                        https://briefingday.com/fan     True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 2: Preprocesa los enlaces\n",
    "\n",
    "Utiliza lo visto en este módulo para transformar los datos para compatibilizarlos con el modelo que queremos entrenar. Segmenta las URLs en partes según sus signos de puntuación, elimina las stopwords, lematiza, etcétera.\n",
    "\n",
    "Asegúrate de dividir convenientemente el conjunto de datos en train y test como hemos visto en lecciones anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transformación categórica a numérica\n",
    "\n",
    "Transformamos la categoría `is_spam` en valores numéricos (0 y 1), ya que este, como la mayoría de los modelos, no trabajan con variables de clase categóricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://briefingday.us8.list-manage.com/unsubs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.hvper.com/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://briefingday.com/m/v4n3i4f3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://briefingday.com/n/20200618/m#commentform</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://briefingday.com/fan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  is_spam\n",
       "0  https://briefingday.us8.list-manage.com/unsubs...        1\n",
       "1                             https://www.hvper.com/        1\n",
       "2                 https://briefingday.com/m/v4n3i4f3        1\n",
       "3   https://briefingday.com/n/20200618/m#commentform        0\n",
       "4                        https://briefingday.com/fan        1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformamos True y False en 1 y 0. Para indicar si es spam o no\n",
    "\n",
    "df['is_spam'] = df['is_spam'].apply(lambda x: 1 if x == True else 0).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Eliminación de valores repetidos\n",
    "\n",
    "Podemos contar fácilmente cuántos casos de cada clase tenemos para analizar si el conjunto de datos está equilibrado o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999 datos y 2 categorías\n",
      "Spam: 696\n",
      "No Spam: 2303\n"
     ]
    }
   ],
   "source": [
    "row, col = df.shape\n",
    "\n",
    "print(f\"{row} datos y {col} categorías\")\n",
    "\n",
    "print(f\"Spam: {len(df.loc[df['is_spam'] == 1])}\")\n",
    "\n",
    "print(f\"No Spam: {len(df.loc[df['is_spam'] == 0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Como podemos ver no está equilibrado porque tenemos 696 mensajes de spam y 2303 mensajes de no spam.\n",
    "\n",
    "Debemos eliminar también los duplicados, si los hubiera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate values in the dataset is 630\n",
      "Después del procesado tenemos...\n",
      "The number of duplicate values in the dataset is 0\n"
     ]
    }
   ],
   "source": [
    "# Obtain the number of duplicate values in our dataset\n",
    "def procesado_duplicados(dataframe):\n",
    "\n",
    "    duplicados = dataframe.duplicated().sum()\n",
    "    print(f\"The number of duplicate values in the dataset is {duplicados}\")\n",
    "\n",
    "    if duplicados > 0:\n",
    "\n",
    "        print(\"Después del procesado tenemos...\")\n",
    "        \n",
    "        dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "        dataframe = dataframe.reset_index(inplace = False, drop = True)\n",
    "\n",
    "        final_duplicados = dataframe.duplicated().sum()\n",
    "\n",
    "        print(f\"The number of duplicate values in the dataset is {final_duplicados}\")\n",
    "\n",
    "    return dataframe \n",
    "\n",
    "df = procesado_duplicados(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que se han eliminado más de 600 registros repetidos.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.3 Procesamiento del texto\n",
    "\n",
    "Para poder entrenar el modelo es necesario aplicar antes un proceso de transformación al texto. Comenzamos transformando el texto a minúsculas y eliminando signos de puntuación y caracteres especiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[https, briefingday, us, list, manage, com, un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[https, www, hvper, com]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[https, briefingday, com, v, i]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[https, briefingday, com, m, commentform]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[https, briefingday, com, fan]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  is_spam\n",
       "0  [https, briefingday, us, list, manage, com, un...        1\n",
       "1                           [https, www, hvper, com]        1\n",
       "2                    [https, briefingday, com, v, i]        1\n",
       "3          [https, briefingday, com, m, commentform]        0\n",
       "4                     [https, briefingday, com, fan]        1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library which can be used to work with Regular Expressions\n",
    "# https://www.w3schools.com/python/python_regex.asp\n",
    "\n",
    "import regex as re\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Eliminar cualquier carácter que no sea una letra (a-z) o un espacio en blanco ( )\n",
    "\n",
    "    text = re.sub(r'[^a-z ]',#  La construcción [^...] indica negación en las expresiones regulares,\n",
    "                             # lo que significa \"coincide con cualquier cosa que no esté en este conjunto\n",
    "                   \" \", # \" \": Esta es la cadena de reemplazo. Cualquier carácter que coincida con el patrón será reemplazado por un espacio.\n",
    "                     text) \n",
    "\n",
    "\n",
    "    # Eliminar espacios en blanco\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', \" \", text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', \" \", text)\n",
    "\n",
    "    # Reducir espacios en blanco múltiples a uno único\n",
    "    text = re.sub(r'\\s+', \" \", text.lower())\n",
    "\n",
    "    # Eliminar tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "    return text.split()\n",
    "\n",
    "df[\"url\"] = df[\"url\"].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es la lematización del texto, que es el proceso de simplificación de las palabras a su forma base o canónica, de manera que palabras con diferentes formas, pero el mismo núcleo semántico, se traten como una sola palabra. Por ejemplo, los verbos \"corriendo\", \"corrió\" y \"corre\" serán lematizados a \"correr\", así como las palabras \"mejores\" y \"mejor\" podrían ser lematizadas a \"bueno\".\n",
    "\n",
    "Además, aprovechando la lematización, eliminaremos también las stopwords, que son palabras que consideramos irrelevantes para el análisis de texto porque aparece con mucha frecuencia en el lenguaje y no aporta información significativa. Existen dos formas: crear nosotros nuestra propia lista de palabras a eliminar o utilizar librerías externas.\n",
    "\n",
    ">NOTE: Ambas tareas las llevaremos a cabo con la librería de Python `NLTK`, que es una de las más importantes en términos del NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Fermin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fermin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Importar WordNet para Lematización\n",
    "# ================================================================\n",
    "\n",
    "download(\"wordnet\") # Descarga el conjunto de datos WordNet necesario para realizar la lematización\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # Crea una instancia del lematizador WordNet para simplificar palabras a sus formas base.\n",
    "\n",
    "\n",
    "# Importar y configurar palabras vacías (stopwords)\n",
    "# ================================================================\n",
    "\n",
    "download(\"stopwords\") # Descarga la lista de palabras vacías en inglés, como, \"the\", \"is\", \"and\"...\n",
    "\n",
    "stop_words = stopwords.words(\"english\") # Obtén la lista de palabras vacías en inglés para filtrar palabras irrelevantes.\n",
    "\n",
    "\n",
    "# Definir una función para lematizar texto\n",
    "# ================================================================\n",
    "def lemmatize_text(words, lemmatizer = lemmatizer):\n",
    "    \"\"\"\n",
    "    Lematiza una lista de palabras y filtra palabras irrelevantes.\n",
    "\n",
    "    Parámetros:\n",
    "    - words: lista de palabras a procesar.\n",
    "    - lemmatizer: instancia del lematizador de WordNet (por defecto usa la creada previamente).\n",
    "\n",
    "    Retorno:\n",
    "    - Lista de palabras lematizadas, sin palabras vacías y con longitud mayor a 3 caracteres.\n",
    "    \"\"\"\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Lematiza cada pañabra en la lista para obtener su forma base (por ejemplo, \"running\" -> \"run\")\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Filtra las palabras vacías para eliminar términos comunes que no aportan significado.\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    # Filtra palabras cuya longitud sea menor o igual a 3 carácteres, ya que suelen ser irrelevantes.\n",
    "\n",
    "\n",
    "    return tokens # Devuelve la lista procesada de palabras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[http, briefingday, list, manage, unsubscribe]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[http, hvper]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[http, briefingday]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[http, briefingday, commentform]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[http, briefingday]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url  is_spam\n",
       "0  [http, briefingday, list, manage, unsubscribe]        1\n",
       "1                                   [http, hvper]        1\n",
       "2                             [http, briefingday]        1\n",
       "3                [http, briefingday, commentform]        0\n",
       "4                             [http, briefingday]        1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.url = df.url.apply(lemmatize_text)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo muy común una vez tenemos los tokens es representarlos en una nube de palabras. Una nube de palabras es una representación visual de las palabras que componen un texto, donde el tamaño de cada palabra indica su frecuencia o importancia en dicho texto.\n",
    "\n",
    "Esta representación visual permite identificar rápidamente los términos o conceptos más relevantes o repetidos en un conjunto de datos, ya que las palabras más frecuentes o significativas sobresalen por su tamaño. Podemos implementarla fácilmente utilizando la librería `wordcloud` para Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
